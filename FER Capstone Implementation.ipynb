{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Emotions Recognition\n",
    "\n",
    "### Domain Background\n",
    "\n",
    "Facial emotions are important factors in human communication that help us understand the intentions of others. In general, people infer the emotional states of other people, such as joy, sadness, and anger, using facial expressions and vocal tone. According to different surveys, verbal components convey one-third of human communication, and nonverbal components convey two-thirds. Among several nonverbal components, by carrying emotional meaning, facial expressions are one of the main information channels in interpersonal communication. Interest in automatic facial emotion recognition (FER) has also been increasing recently with the rapid development of artificial intelligent techniques, including in human-computer interaction (HCI), virtual reality (VR), augment reality (AR), advanced driver assistant systems (ADASs), and entertainment. Although various sensors such as an electromyograph (EMG), electrocardiogram (ECG), electroencephalograph (EEG), and camera can be used for FER inputs, a camera is the most promising type of sensor because it provides the most informative clues for FER and does not need to be worn.\n",
    "\n",
    "My journey to decide on this project was exciting. My motive was to prove the utility of Deep neural nets in the contemporary research. Facial emotional recoginition/ pattern recognintion had been in research since long. The following acaemic papers were very helpful in  \n",
    "\n",
    "1. [Giving a historic overview of research in Facial Emotional Recognition](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5856145/)\n",
    "2. [Deciding on a posed dataset with seven different emotions](http://www.consortium.ri.cmu.edu/data/ck/CK+/CVPR2010_CK.pdf)\n",
    "3. [Developing a baseline algorithm](https://pdfs.semanticscholar.org/9bf2/c915943cb74add761ec4636f323337022a97.pdf)\n",
    "4. [Improving the Facial Emotions Recognition using Deep Convolutional Neuralnets](https://arxiv.org/pdf/1509.05371v2.pdf)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "\n",
    " The objective of this project is to showcase two different solutions in solving the problem of Facial emotional recognition from a posed dataset. Both the solutions are based on the problem space of supervised learning. But the first solution I propose is more involved and has more human interference than the second solution which uses state of art artificial neuralnets. The goal is to compare the two approaches using a performance metric - i.e how well the supervised learning model detects the expression posed in a still image. The posed dataset has labels associated with it. The labels define the most probable emotion. After running our two supervised learning model, we use accuracy score as the performance metric to decide how well the model has performed.\n",
    " \n",
    "accuracy score = A ratio of # of correctly predicted emotions in images / total number of images.\n",
    "\n",
    "### Datasets and Inputs\n",
    "\n",
    "I use [Cohn-Kanade dataset](http://www.consortium.ri.cmu.edu/ckagree/). This dataset has been introduced by [Lucey et al](http://www.pitt.edu/~jeffcohn/CVPR2010_CK+2.pdf). 210 persons, aged 18 to 50, have been recorded depicting emotions.Out of 210 people, only 123 subjects gave posed facial expression. This dataset contains the recordings of their emotions. Both female and male persons are present from different background. 81 % Euro-Americans and 13%  are Afro-Americans. The images are of size 640 * 490 pixels as well as 640 * 480 pixels.  They are both grayscale and colored. in total there are 593 emotion-labeled sequences. There are seven different emotions that are depicted. They are:\n",
    "\n",
    "0. 0=Neutral\n",
    "1. 1=Anger\n",
    "2. 2=Contempt\n",
    "3. 3=Disgust\n",
    "4. 4=Fear\n",
    "5. 5=Happy\n",
    "6. 6=Sadness\n",
    "7. 7=Surprise\n",
    "\n",
    "The images within each subfolder may have an image sequence of the subject. The first image in the sequence starts with a neutral face and the final image in the sub folder has the actual emotion. So from each subfolder ( image sequence), I have to extract two images,  the neutral face and final image with an emotion. ONLY 327 of the 593 sequences have emotion sequences. This is because these are the only ones the fit the prototypic definition. Also all these files are only one single emotion file. I have to preprocess this dataset to make it as an uniform input. I will make sure the images are all of same size and atmost it has one face depicting the emotion for now. After detecting the face in the image, I will convert the image to grayscale image, crop it and save it. I will use OpenCV to automate face finding process. OpenCv comes up with 4 different pre-trained  classifiers. I will use all of them to find the face in the image and abort the process when the face is identified. These identified, cropped, resize image becomes input feature. The emotion labels are the output.\n",
    "\n",
    "\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "![FER](https://drive.google.com/uc?export=view&id=1dvJBlYr76j7VF6JN2ew87paZF6svoSrz)\n",
    "\n",
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from shutil import copyfile\n",
    "emotions = [\"neutral\", \"anger\", \"contempt\", \"disgust\", \"fear\", \"happy\", \"sadness\", \"surprise\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = glob.glob(\"source_emotion/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_file_dict={}\n",
    "for obs in observations:\n",
    "    obs_id = str(obs[-4:])\n",
    "    neutral_file_dict[obs_id]=list()\n",
    "    emotions_folders = \"{0}/*\".format(str(obs))\n",
    "    emotions_sessions =  glob.glob(emotions_folders)\n",
    "    for each_emotion_session in emotions_sessions:\n",
    "        emotion_sequence_folder = \"{0}/*\".format(each_emotion_session)\n",
    "        emotion_sequence_files = glob.glob(emotion_sequence_folder)\n",
    "        # if emotion is identifed\n",
    "        # map the peak frame image to the specified emotion folder in the dataset\n",
    "        # map the first frame image to the neutral folder in the dataset.\n",
    "        for emotion_output in emotion_sequence_files:\n",
    "            emotion_seq_no = emotion_output[20:23]\n",
    "            # read the emotion in the file\n",
    "            file = open(emotion_output, 'r')\n",
    "            emotion= int(float(file.readline()))\n",
    "            peak_frame_emotion = sorted(glob.glob(\"source_images/{0}/{1}/*\".format(obs_id,emotion_seq_no)))[-1]\n",
    "            first_frame_emotion = sorted(glob.glob(\"source_images/{0}/{1}/*\".format(obs_id,emotion_seq_no)))[0]\n",
    "            neutral_file_dict[obs_id].append(first_frame_emotion)\n",
    "            peak_emotion_img_name = \"pre_dataset/{0}/{1}\".format(emotions[emotion], peak_frame_emotion[23:])\n",
    "            # now copy peak frame to respective folders in dataset\n",
    "            copyfile(peak_frame_emotion, peak_emotion_img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123)\n",
    "for key in neutral_file_dict.keys():\n",
    "    if len(neutral_file_dict[key])>0:\n",
    "        # randomly copy neutral frames to neutral folder\n",
    "        source_file = random.choice(neutral_file_dict[key])\n",
    "        neutral_emotion_img_name = \"pre_dataset/neutral/{0}\".format(source_file[23:])\n",
    "        copyfile(source_file, neutral_emotion_img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from http://www.paulvangent.com/2016/04/01/emotion-recognition-with-python-opencv-and-a-face-dataset/\n",
    "import cv2\n",
    "faceDet = cv2.CascadeClassifier(\"haarcascades/haarcascade_frontalface_default.xml\")\n",
    "faceDet_two = cv2.CascadeClassifier(\"haarcascades/haarcascade_frontalface_alt2.xml\")\n",
    "faceDet_three = cv2.CascadeClassifier(\"haarcascades/haarcascade_frontalface_alt.xml\")\n",
    "faceDet_four = cv2.CascadeClassifier(\"haarcascades/haarcascade_frontalface_alt_tree.xml\")\n",
    "def detect_faces(emotion):\n",
    "    files = glob.glob(\"pre_dataset/{0}/*\".format(emotion)) #Get list of all images with emotion\n",
    "    filenumber = 0\n",
    "    for f in files:\n",
    "        frame = cv2.imread(f) #Open image\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) #Convert image to grayscale\n",
    "        #Detect face using 4 different classifiers\n",
    "        face = faceDet.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_two = faceDet_two.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_three = faceDet_three.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        face_four = faceDet_four.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(5, 5), flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        #Go over detected faces, stop at first detected face, return empty if no face.\n",
    "        if len(face) == 1:\n",
    "            facefeatures = face\n",
    "        elif len(face_two) == 1:\n",
    "            facefeatures = face_two\n",
    "        elif len(face_three) == 1:\n",
    "            facefeatures = face_three\n",
    "        elif len(face_four) == 1:\n",
    "            facefeatures = face_four\n",
    "        else:\n",
    "            facefeatures = \"\"\n",
    "        #Cut and save face\n",
    "        for (x, y, w, h) in facefeatures: #get coordinates and size of rectangle containing face\n",
    "            gray = gray[y:y+h, x:x+w] #Cut the frame to size\n",
    "            try:\n",
    "                out = cv2.resize(gray, (350, 350)) #Resize face so all images have same size\n",
    "                cv2.imwrite(\"dataset/{0}/{1}.png\".format(emotion, filenumber), out) #Write image\n",
    "            except:\n",
    "               pass #If error, pass file\n",
    "        filenumber += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emotion in emotions:\n",
    "    detect_faces(emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"pre_dataset/{0}/*\".format(\"neutral\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pre_dataset/neutral/S505_002_00000001.png',\n",
       " 'pre_dataset/neutral/S102_002_00000001.png',\n",
       " 'pre_dataset/neutral/S029_001_00000001.png',\n",
       " 'pre_dataset/neutral/S059_002_00000001.png',\n",
       " 'pre_dataset/neutral/S062_004_00000001.png',\n",
       " 'pre_dataset/neutral/S115_008_00000001.png',\n",
       " 'pre_dataset/neutral/S028_001_00000001.png',\n",
       " 'pre_dataset/neutral/S014_001_00000001.png',\n",
       " 'pre_dataset/neutral/S160_006_00000001.png',\n",
       " 'pre_dataset/neutral/S071_002_00000001.png',\n",
       " 'pre_dataset/neutral/S087_007_00000001.png',\n",
       " 'pre_dataset/neutral/S068_003_00000001.png',\n",
       " 'pre_dataset/neutral/S502_004_00000001.png',\n",
       " 'pre_dataset/neutral/S155_002_00000001.png',\n",
       " 'pre_dataset/neutral/S097_001_00000001.png',\n",
       " 'pre_dataset/neutral/S134_004_00000001.png',\n",
       " 'pre_dataset/neutral/S098_004_00000001.png',\n",
       " 'pre_dataset/neutral/S034_003_00000001.png',\n",
       " 'pre_dataset/neutral/S065_004_00000001.png',\n",
       " 'pre_dataset/neutral/S073_001_00000001.png',\n",
       " 'pre_dataset/neutral/S053_004_00000001.png',\n",
       " 'pre_dataset/neutral/S035_005_00000001.png',\n",
       " 'pre_dataset/neutral/S091_003_00000001.png',\n",
       " 'pre_dataset/neutral/S060_005_00000001.png',\n",
       " 'pre_dataset/neutral/S148_002_00000001.png',\n",
       " 'pre_dataset/neutral/S094_001_00000001.png',\n",
       " 'pre_dataset/neutral/S100_005_00000001.png',\n",
       " 'pre_dataset/neutral/S077_006_00000001.png',\n",
       " 'pre_dataset/neutral/S113_008_00000001.png',\n",
       " 'pre_dataset/neutral/S105_008_00000001.png',\n",
       " 'pre_dataset/neutral/S084_002_00000001.png',\n",
       " 'pre_dataset/neutral/S022_006_00000001.png',\n",
       " 'pre_dataset/neutral/S136_001_00000001.png',\n",
       " 'pre_dataset/neutral/S116_001_00000001.png',\n",
       " 'pre_dataset/neutral/S135_001_00000001.png',\n",
       " 'pre_dataset/neutral/S129_002_00000001.png',\n",
       " 'pre_dataset/neutral/S111_001_00000001.png',\n",
       " 'pre_dataset/neutral/S506_004_00000001.png',\n",
       " 'pre_dataset/neutral/S139_002_00000001.png',\n",
       " 'pre_dataset/neutral/S101_002_00000001.png',\n",
       " 'pre_dataset/neutral/S126_004_00000001.png',\n",
       " 'pre_dataset/neutral/S130_013_00000001.png',\n",
       " 'pre_dataset/neutral/S046_001_00000001.png',\n",
       " 'pre_dataset/neutral/S078_004_00000001.png',\n",
       " 'pre_dataset/neutral/S057_001_00000001.png',\n",
       " 'pre_dataset/neutral/S063_001_00000001.png',\n",
       " 'pre_dataset/neutral/S054_003_00000001.png',\n",
       " 'pre_dataset/neutral/S085_004_00000001.png',\n",
       " 'pre_dataset/neutral/S999_001_00000001.png',\n",
       " 'pre_dataset/neutral/S026_001_00000001.png',\n",
       " 'pre_dataset/neutral/S082_005_00000001.png',\n",
       " 'pre_dataset/neutral/S124_007_00000001.png',\n",
       " 'pre_dataset/neutral/S128_004_00000001.png',\n",
       " 'pre_dataset/neutral/S076_006_00000001.png',\n",
       " 'pre_dataset/neutral/S503_006_00000001.png',\n",
       " 'pre_dataset/neutral/S044_001_00000001.png',\n",
       " 'pre_dataset/neutral/S066_002_00000001.png',\n",
       " 'pre_dataset/neutral/S069_003_00000001.png',\n",
       " 'pre_dataset/neutral/S005_001_00000001.png',\n",
       " 'pre_dataset/neutral/S137_001_00000001.png',\n",
       " 'pre_dataset/neutral/S895_002_00000001.png',\n",
       " 'pre_dataset/neutral/S501_006_00000001.png',\n",
       " 'pre_dataset/neutral/S064_001_00000001.png',\n",
       " 'pre_dataset/neutral/S099_007_00000001.png',\n",
       " 'pre_dataset/neutral/S125_008_00000001.png',\n",
       " 'pre_dataset/neutral/S050_004_00000001.png',\n",
       " 'pre_dataset/neutral/S095_001_00000001.png',\n",
       " 'pre_dataset/neutral/S088_004_00000001.png',\n",
       " 'pre_dataset/neutral/S032_006_00000001.png',\n",
       " 'pre_dataset/neutral/S011_003_00000001.png',\n",
       " 'pre_dataset/neutral/S051_003_00000001.png',\n",
       " 'pre_dataset/neutral/S037_006_00000001.png',\n",
       " 'pre_dataset/neutral/S106_006_00000001.png',\n",
       " 'pre_dataset/neutral/S122_001_00000001.png',\n",
       " 'pre_dataset/neutral/S056_004_00000001.png',\n",
       " 'pre_dataset/neutral/S067_006_00000001.png',\n",
       " 'pre_dataset/neutral/S093_004_00000001.png',\n",
       " 'pre_dataset/neutral/S119_008_00000001.png',\n",
       " 'pre_dataset/neutral/S151_002_00000001.png',\n",
       " 'pre_dataset/neutral/S010_002_00000001.png',\n",
       " 'pre_dataset/neutral/S107_001_00000001.png',\n",
       " 'pre_dataset/neutral/S156_002_00000001.png',\n",
       " 'pre_dataset/neutral/S117_003_00000001.png',\n",
       " 'pre_dataset/neutral/S042_001_00000001.png',\n",
       " 'pre_dataset/neutral/S149_002_00000001.png',\n",
       " 'pre_dataset/neutral/S147_002_00000001.png',\n",
       " 'pre_dataset/neutral/S127_010_00000001.png',\n",
       " 'pre_dataset/neutral/S072_006_00000001.png',\n",
       " 'pre_dataset/neutral/S081_002_00000001.png',\n",
       " 'pre_dataset/neutral/S154_002_00000001.png',\n",
       " 'pre_dataset/neutral/S074_005_00000001.png',\n",
       " 'pre_dataset/neutral/S083_003_00000001.png',\n",
       " 'pre_dataset/neutral/S070_003_00000001.png',\n",
       " 'pre_dataset/neutral/S090_007_00000001.png',\n",
       " 'pre_dataset/neutral/S092_004_00000001.png',\n",
       " 'pre_dataset/neutral/S157_002_00000001.png',\n",
       " 'pre_dataset/neutral/S108_008_00000001.png',\n",
       " 'pre_dataset/neutral/S158_002_00000001.png',\n",
       " 'pre_dataset/neutral/S112_005_00000001.png',\n",
       " 'pre_dataset/neutral/S132_003_00000001.png',\n",
       " 'pre_dataset/neutral/S114_006_00000001.png',\n",
       " 'pre_dataset/neutral/S110_001_00000001.png',\n",
       " 'pre_dataset/neutral/S138_007_00000001.png',\n",
       " 'pre_dataset/neutral/S131_010_00000001.png',\n",
       " 'pre_dataset/neutral/S089_003_00000001.png',\n",
       " 'pre_dataset/neutral/S080_001_00000001.png',\n",
       " 'pre_dataset/neutral/S055_006_00000001.png',\n",
       " 'pre_dataset/neutral/S133_010_00000001.png',\n",
       " 'pre_dataset/neutral/S061_001_00000001.png',\n",
       " 'pre_dataset/neutral/S075_008_00000001.png',\n",
       " 'pre_dataset/neutral/S058_001_00000001.png',\n",
       " 'pre_dataset/neutral/S109_003_00000001.png',\n",
       " 'pre_dataset/neutral/S045_004_00000001.png',\n",
       " 'pre_dataset/neutral/S096_004_00000001.png',\n",
       " 'pre_dataset/neutral/S504_006_00000001.png',\n",
       " 'pre_dataset/neutral/S052_004_00000001.png',\n",
       " 'pre_dataset/neutral/S079_002_00000001.png',\n",
       " 'pre_dataset/neutral/S086_001_00000001.png']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer",
   "language": "python",
   "name": "fer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
