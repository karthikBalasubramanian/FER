{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Facial Emotions Recognition\n",
    "\n",
    "Karthik Balasubramanian \n",
    "May 16th, 2019\n",
    "\n",
    "## I. Definition\n",
    "\n",
    "### Project Overview\n",
    "Facial emotions are important factors in human communication that help us understand the intentions of others. In general, people infer the emotional states of other people, such as joy, sadness, and anger, using facial expressions and vocal tone. According to different surveys, verbal components convey one-third of human communication, and nonverbal components convey two-thirds. Among several nonverbal components, by carrying emotional meaning, facial expressions are one of the main information channels in interpersonal communication. Interest in automatic facial emotion recognition (FER) has also been increasing recently with the rapid development of artificial intelligent techniques, including in human-computer interaction (HCI), virtual reality (VR), augment reality (AR), advanced driver assistant systems (ADASs), and entertainment. Although various sensors such as an electromyograph (EMG), electrocardiogram (ECG), electroencephalograph (EEG), and camera can be used for FER inputs, a camera is the most promising type of sensor because it provides the most informative clues for FER and does not need to be worn.\n",
    "\n",
    "My journey to decide on this project was exciting. My motive was to compare the performances of Deep neural nets in the contemporary research to heuristically learned pattern recognition methods. Facial emotional recoginition/ pattern recognintion had been in research since long. The following academic papers were very helpful in\n",
    "\n",
    "1. [Giving a historic overview of research in Facial Emotional Recognition](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5856145/)\n",
    "2. [Deciding on a posed dataset with seven different emotions](http://www.consortium.ri.cmu.edu/data/ck/CK+/CVPR2010_CK.pdf)\n",
    "3. [Developing a baseline algorithm](https://pdfs.semanticscholar.org/9bf2/c915943cb74add761ec4636f323337022a97.pdf)\n",
    "4. [Improving the Facial Emotions Recognition using Deep Convolutional Neuralnets](https://arxiv.org/pdf/1509.05371v2.pdf)\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "The objective of this project is to showcase two different solutions in solving the problem of Facial emotional recognition from a posed dataset. Both the solutions are based on the problem space of supervised learning. But the first solution I propose is more involved and has more human interference than the second solution which uses state of art artificial neuralnets. The goal is to compare the two approaches using a performance metric - i.e how well the supervised learning model detects the expression posed in a still image. The posed dataset has labels associated with it. The labels define the most probable emotion. After running our two different supervised learning model solutions, I will compare the performance of these solutions using the metrics I define in the next section.\n",
    " \n",
    " Here is the basic structure in attempting to solve the problem.\n",
    "\n",
    "![FER](https://drive.google.com/uc?export=view&id=1dvJBlYr76j7VF6JN2ew87paZF6svoSrz)\n",
    "\n",
    "1. Data preprocessing - Steps done to create a clean dataset\n",
    "2. Feature Extraction and Normalization methods - The supervised learning approach uses some hand crafted shape prediction methods while the deep neuralnets have data normalization steps. Both these methods are followed by creation of train, valid test set splits\n",
    "3. Model Application - Model train and test phase.\n",
    "4. Model Performance - Performance of the models are compared with the metrics defined below.\n",
    "\n",
    "\n",
    "### Metrics\n",
    "This is a problem of classifying an image to its right emotion. A multi-class classification problem. Our immediate resort to any such problem will be\n",
    "\n",
    "1. Accuracy - is defined as \n",
    "\n",
    "      $(True Positives + True Negatives)$  /$(True Positives + True Negatives+ False Positives + False Negatives)$  \n",
    "\n",
    "But when the dataset is not uniformly distributed across classes we use cross entropy loss. \n",
    "\n",
    "2. Cross entropy loss - Which compares the loss between a vector and a true label in the SVM model.In the deep networks it compares the distance between two vectors. The predicted vector has the probabilities of all the classes. The true vector is a one-hot encoded vector of output labels. The lesser the cross entropy the better the model is. We should also aim at minimizing this loss (i.e closer to 0). In deep learning models, a softmax layer is introduced in the final layer to get the probability vector of classes from the matrix multiplicated computations.\n",
    " \n",
    "      $CE = -\\sum_{i}^{C}t_{i} log (s_{i})$\n",
    "\n",
    "     Where $t_{i}$  and $s_{i}$ are the groundtruth (label or one hot      encoded vector) and the probability for each class $i$ in $C$          classes. \n",
    "\n",
    "\n",
    "## II. Analysis\n",
    "\n",
    "### Data Exploration\n",
    "\n",
    "I use [Cohn-Kanade dataset](http://www.consortium.ri.cmu.edu/ckagree/). This dataset has been introduced by [Lucey et al](http://www.pitt.edu/~jeffcohn/CVPR2010_CK+2.pdf). 210 persons, aged 18 to 50, have been recorded depicting emotions.Out of 210 people, only 123 subjects gave posed facial expression. This dataset contains the recordings of their emotions. Both female and male persons are present from different background. 81 % Euro-Americans and 13%  are Afro-Americans. The images are of size 640 * 490 pixels as well as 640 * 480 pixels.  They are both grayscale and colored. in total there are 593 emotion-labeled sequences. There are seven different emotions that are depicted. They are:\n",
    "\n",
    "0. 0=Neutral\n",
    "1. 1=Anger\n",
    "2. 2=Contempt\n",
    "3. 3=Disgust\n",
    "4. 4=Fear\n",
    "5. 5=Happy\n",
    "6. 6=Sadness\n",
    "7. 7=Surprise\n",
    "\n",
    "The images within each subfolder may have an image sequence of the subject. The first image in the sequence starts with a neutral face and the final image in the sub folder has the actual emotion. So from each subfolder ( image sequence), I have to extract two images,  the neutral face and final image with an emotion. ONLY 327 of the 593 sequences have emotion sequences. This is because these are the only ones the fit the prototypic definition. Also all these files are only one single emotion file. I have to preprocess this dataset to make it as an uniform input. I will make sure the images are all of same size and atmost it has one face depicting the emotion for now. After detecting the face in the image, I will convert the image to grayscale image, crop it and save it. I will use OpenCV to automate face finding process. OpenCv comes up with 4 different pre-trained  classifiers. I will use all of them to find the face in the image and abort the process when the face is identified. These identified, cropped, resize image becomes input feature. The emotion labels are the output.\n",
    "\n",
    "Here is the statistical report of the dataset after I do all the preprocessing.\n",
    "\n",
    "![Dataset_stats](https://drive.google.com/uc?export=view&id=1stMpFbmTcgN8pcR057XZd7W0GRka9IMg)\n",
    "\n",
    "**Abnormalities in the Dataset**  \n",
    "I am aware of the following abnormalities in the dataset. but I am still going with the dataset.\n",
    "\n",
    "1. The dataset is not uniformly distributed. So using Accuracy as a metric will lead us into Accuracy Paradox. Hence I have introduced another metric called log loss or categorical cross entropy as another metric.\n",
    "\n",
    "2. We have a very small dataset per emotion after preprocessing. We had to drop many image sequences as they had no labels.\n",
    "\n",
    "\n",
    "### Exploratory Visualization\n",
    "\n",
    "Before we jump into finding solutions to the problem by defining the algorithms, we have to understand the feature extraction process. I will be using standard preprocessing methods in deep learning models as I plan to use transfer learning. I transfer learn the emotions from state of art models like VGG16 and Xception. They have their own pre processing methods. All I have to do is present the image in a format (face identified, cropped, resized image) to get preprocessed to a format that these state of art models want to. But there is a significant education needed to understand the feature extraction process to implement the baseline models.\n",
    "\n",
    "#### Understanding  Feature Extraction Process In Baseline models\n",
    "\n",
    "![ActionUnit](https://drive.google.com/uc?export=view&id=1xMnDLiOz5_MxnDCMzJ5mKjM2yMXNNzCR)\n",
    "\n",
    "The Feature Extraction process has 2 different phases.\n",
    "\n",
    "1. Finding the face - Libraries like dlib has functions like `get_frontal_face_detector` which is handy to identify the face region\n",
    "2. Extracting the features in the face - This is where most of the research in the past has gone into. It has been done so far by realizing through manual interference. One of the method is called Facial Action Coding System (FACS) which describes Facial expression using Action Units (AU). An Action Unit is a facial action like \"raising the Inner eyebrow\". Multiple Activation units when combined expresses the emotion in the underlying face. An example is provided below.\n",
    "\n",
    "![FACS](https://drive.google.com/uc?export=view&id=14krm8krZudg4ekBOva4XYKcnAPoOM15j)\n",
    "\n",
    "I use dlib's `shape_predictor` and its learned landmark predictor `shape_predictor_68_face_landmarks.bat` to extract AUs.\n",
    "\n",
    "The below image helps us understand the facial action units.\n",
    "\n",
    "![FACS_WITH_FACE](https://drive.google.com/uc?export=view&id=1jUHU36pi5MB-UBoaxmyn4RLQN6UAq_h-)\n",
    "\n",
    "\n",
    "![FACS_WITHOUT_FACE](https://drive.google.com/uc?export=view&id=1BZJ6uBFcn7KfEvuC8YyZqiMyoY2b8jlW)\n",
    "\n",
    "The `shape_predictor_68_face_landmarks` above have extracted 67 points in any face in both X and Y axis from the image presented. This X and Y points when combined becomes a Facial Landmark. They describe the position of all the “moving parts” of the depicted face, the things you use to express an emotion. The good thing about extracting facial landmark is that I will be extracting very important information from the image to use it and classify an emotion. But,\n",
    "\n",
    "There are some problems when we directly capture these facial landmarks.\n",
    "\n",
    "- They may change as face moves to different parts of the frame. An image could be expressing the same emotion in the top left pixel as in the bottom right pixel of another image, but the resulting coordinate matrix would express different numerical ranges and hence the two images can be classfied to different emotion instead of the same emotion.Therefore we need a location invariant coordinate matrix to help us classify an emotion.\n",
    "\n",
    "The solution to this problem is derived in the following way.\n",
    "\n",
    "1. Find the center of the shap predictor vector\n",
    "2. Calculate the distance between all the shape predictor points to their center\n",
    "3. Calculate the angle at which these points find themselves relative to the center point. \n",
    "\n",
    "\n",
    "What we now have is the relationship between all the points with the center point and how they are relatively positioned in the 2D space.Each tuple will have the following values `<x, y, distance_from_center, angle_relative_to_center>`. This additional information to each coordinate makes it location invariant. i.e There is a way to derive these points in the 2D system. These becomes features to our baseline models.\n",
    "\n",
    "Example feature vector\n",
    "\n",
    "```[34.0, 172.0, 143.73715832690573, -163.42042572345252]```\n",
    "\n",
    "### Algorithms and Techniques - Baseline\n",
    "\n",
    "I have chosen Support vector machines (SVMs) to map the different facial features to their emotions. SVMs attempt to find the hyperplane that maximizes the margin between positive and negative observations for a specified emotion class. Therefore its also called Maximum margin classifier.\n",
    "  \n",
    "We use libSVM which uses one vs one classifier. i.e It will create $ (K * (K-1))/2 $ binary classifiers in total - where K here is number of classes 8. A total of 28 binary classfiers are created. \n",
    "  \n",
    "\n",
    "**Linear SVM**\n",
    "> Definitions taken from [Cohn-Kanade+ paper](http://www.pitt.edu/~jeffcohn/CVPR2010_CK+2.pdf)\n",
    "\n",
    "A linear SVM classification decision is made for an unlabeled test observation `x*` by,\n",
    "\n",
    "$w^Tx^* >^{true}  b$  \n",
    "$w^Tx^* <=^{false} b$  \n",
    "\n",
    "where w is the vector normal to the separating hyperplane and b is the bias. Both w and b are estimated so that they minimize the risk of a train-set, thus avoiding the possibility of overfitting to the training data. Typically, w is not defined explicitly, but through a linear sum of support vectors.\n",
    "\n",
    "\n",
    "**Polynomial SVM**\n",
    "\n",
    "The kernel methods in SVM are used when we don't have lineraly seperable data. Kernel methods transform the data to higher dimension to make them seperable. By default, we have our feature set expressed to a 3 degree polynomial.\n",
    "\n",
    "\n",
    "##### Parameters passed for baseline models\n",
    "1. `random_state` - This is like seed value for model to return same response everytime we run it.\n",
    "2. `probability` - We have asked the model to provide probability scores on different categories.\n",
    "3. `kernel` - linear/ poly\n",
    "4. `tolerance` - Tolerence for stopping criteria for the model.\n",
    "\n",
    "### Algorithms and Techniques - Deep Neural Nets\n",
    "\n",
    "**Transfer Learning**  \n",
    "I have implemented transfer learned Neural Nets. My goal here is to do very minimal work, reuse the wealthy knowledge of deep networks that have been proven before for image detection. The concepts are the same, but the task to identify is only different.\n",
    "\n",
    "Here are the steps I am planning to take to make my model more generic and expressive to capture the emotions.\n",
    "\n",
    "  - Feature Extraction - Preprocess the images. Convert all the images to 4 dimensional tensors.\n",
    "  - Get Bottlenect features using the State-of-Art network weights.\n",
    "  - Create a new model to train the bottleneck features to capture the emotions\n",
    "  - predict the emotions from Bottleneck test features.\n",
    "  - Analyze the confusion matrix\n",
    "  - Compare performances with Baseline model results.\n",
    "  \n",
    "##### State-of-Art networks used\n",
    "1. VGG16\n",
    "2. Xception\n",
    "\n",
    "##### Model implemented to train bottleneck features\n",
    "*VGG16 bottleneck based model*\n",
    "\n",
    "```______________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #  \n",
    "===============================================================\n",
    "global_average_pooling2d_1 ( (None, 512)               0         \n",
    "_______________________________________________________________\n",
    "dense_1 (Dense)              (None, 8)                 4104      \n",
    "===============================================================\n",
    "Total params: 4,104\n",
    "Trainable params: 4,104\n",
    "Non-trainable params: 0\n",
    "_______________________________________________________________\n",
    "``` \n",
    "*Xception bottleneck based model*\n",
    "\n",
    "```______________________________________________________________\n",
    "Layer (type)                 Output Shape              Param # \n",
    "===============================================================\n",
    "global_average_pooling2d_2 ( (None, 2048)              0         \n",
    "_______________________________________________________________\n",
    "dense_2 (Dense)              (None, 8)                 16392    \n",
    "===============================================================\n",
    "Total params: 16,392\n",
    "Trainable params: 16,392\n",
    "Non-trainable params: 0\n",
    "_______________________________________________________________\n",
    "```\n",
    "##### Parameters passed for deep neural nets\n",
    "\n",
    "1. Train set - Shuffle and randomly choose 80 % of training data\n",
    "2. Validation set - Shuffle and randomly choose 20 % of training data\n",
    "3. `batch_size` = 20 (Use 20 examples to train, backpropagate and learn every epoch)\n",
    "4. `epochs` - I have used 20 epochs. That is 20 instances of training.\n",
    "5. `callbacks` - ModelCheckpoint to save the best epoch weight\n",
    "6. `gradient optimizer` - Adam\n",
    "7. `loss` - categorical crossentropy\n",
    "\n",
    "### Benchmark\n",
    "In this section, you will need to provide a clearly defined benchmark result or threshold for comparing across performances obtained by your solution. The reasoning behind the benchmark (in the case where it is not an established result) should be discussed. Questions to ask yourself when writing this section:\n",
    "- _Has some result or value been provided that acts as a benchmark for measuring performance?_\n",
    "- _Is it clear how this result or value was obtained (whether by data or by hypothesis)?_\n",
    "\n",
    "\n",
    "The Baseline models perform pretty well. They are applied after doing handcrafted feature extractions specified above in the Exploratory Visualization section. \n",
    "\n",
    "| Baseline Algorithm | Cross Entropy Loss - Train | Cross Entropy Loss - Test | Accuracy - Train | Accuracy - Test |\n",
    "|--------------------|----------------------------|---------------------------|------------------|-----------------|\n",
    "| Linear SVM         | 0.31                       | 0.57                      | 1.0              | 0.84            |\n",
    "| Polynomial SVM     | 0.31                       | 0.61                      | 1.0              | 0.81            |\n",
    "\n",
    "\n",
    "I chose Linear SVM as our baseline model and I want to see if the transfer learned neural nets are performing better in terms of getting lower Cross Entropy loss in the test set and higher test accuracy.\n",
    "\n",
    "**Linear SVM Test data Confusion Matrix**\n",
    "\n",
    "![LIN_SVM](https://drive.google.com/uc?export=view&id=1FYxH9I390DgcGKJ0nz9a7eMq_e4M4qmQ)\n",
    "\n",
    "\n",
    "\n",
    "## III. Methodology\n",
    "_(approx. 3-5 pages)_\n",
    "\n",
    "### Data Preprocessing\n",
    "In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:\n",
    "- _If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?_\n",
    "- _Based on the **Data Exploration** section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?_\n",
    "- _If no preprocessing is needed, has it been made clear why?_\n",
    "\n",
    "### Implementation\n",
    "In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:\n",
    "- _Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?_\n",
    "- _Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?_\n",
    "- _Was there any part of the coding process (e.g., writing complicated functions) that should be documented?_\n",
    "\n",
    "### Refinement\n",
    "In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:\n",
    "- _Has an initial solution been found and clearly reported?_\n",
    "- _Is the process of improvement clearly documented, such as what techniques were used?_\n",
    "- _Are intermediate and final solutions clearly reported as the process is improved?_\n",
    "\n",
    "\n",
    "## IV. Results\n",
    "_(approx. 2-3 pages)_\n",
    "\n",
    "### Model Evaluation and Validation\n",
    "In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:\n",
    "- _Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?_\n",
    "- _Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?_\n",
    "- _Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?_\n",
    "- _Can results found from the model be trusted?_\n",
    "\n",
    "### Justification\n",
    "In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:\n",
    "- _Are the final results found stronger than the benchmark result reported earlier?_\n",
    "- _Have you thoroughly analyzed and discussed the final solution?_\n",
    "- _Is the final solution significant enough to have solved the problem?_\n",
    "\n",
    "\n",
    "## V. Conclusion\n",
    "_(approx. 1-2 pages)_\n",
    "\n",
    "### Free-Form Visualization\n",
    "In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:\n",
    "- _Have you visualized a relevant or important quality about the problem, dataset, input data, or results?_\n",
    "- _Is the visualization thoroughly analyzed and discussed?_\n",
    "- _If a plot is provided, are the axes, title, and datum clearly defined?_\n",
    "\n",
    "### Reflection\n",
    "In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:\n",
    "- _Have you thoroughly summarized the entire process you used for this project?_\n",
    "- _Were there any interesting aspects of the project?_\n",
    "- _Were there any difficult aspects of the project?_\n",
    "- _Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?_\n",
    "\n",
    "### Improvement\n",
    "In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:\n",
    "- _Are there further improvements that could be made on the algorithms or techniques you used in this project?_\n",
    "- _Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?_\n",
    "- _If you used your final solution as the new benchmark, do you think an even better solution exists?_\n",
    "\n",
    "-----------\n",
    "\n",
    "**Before submitting, ask yourself. . .**\n",
    "\n",
    "- Does the project report you’ve written follow a well-organized structure similar to that of the project template?\n",
    "- Is each section (particularly **Analysis** and **Methodology**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?\n",
    "- Would the intended audience of your project be able to understand your analysis, methods, and results?\n",
    "- Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?\n",
    "- Are all the resources used for this project correctly cited and referenced?\n",
    "- Is the code that implements your solution easily readable and properly commented?\n",
    "- Does the code execute without error and produce results similar to those reported?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer",
   "language": "python",
   "name": "fer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
